{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K邻近算法的原理\n",
    "- 如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。\n",
    "- 来源：KNN算法最早是由Cover和Hart提出的一种分类算法\n",
    "- 两个样本的距离可以有多种算法，比如欧式距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "# import datasets\n",
    "# 加载内置的数据集，格式为[n_samples * n_features]的二维numpy.ndarray数组\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris.data.shape\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sklearn里的估计器：实现算法的API\n",
    "1. 用于分类：\n",
    "    - sklearn.neighbors k近邻算法\n",
    "    - sklearn.naive_bayes 贝叶斯算法\n",
    "    - sklearn.linear_model.LogisticRegression 逻辑回归\n",
    "2. 用于回归：\n",
    "    - sklearn.linear_model.LinearRegression 线性回归\n",
    "    - sklearn.linear_model.Ridge 岭回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K邻近算法的API\n",
    "```python\n",
    "sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, algorithm='auto')\n",
    "    n_neighbors: int，可选，默认为5.默认查询的邻居数\n",
    "    algorithm：{'auto', 'ball_tree', 'kd_tree', 'brute'}.可选用于计算最近邻居的算法：\n",
    "        ‘ball_tree’将会使用 BallTree，\n",
    "        ‘kd_tree’将使用 KDTree。\n",
    "        ‘auto’将尝试根据传递给fit方法的值来决定最合适的算法。 (不同实现方式影响效率)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 模型的选择\n",
    "- 超参数：需要手动指定的参数\n",
    "- 交叉验证：将数据分成N份，其中一份作为预测集，其余的作为测试集。进行N次测试，每次都更换不同的测试集和验证集，取平均值作为最终结果\n",
    "- 网格搜索：一个模型可能有多个超算数，预设几种超参数组合，每种组合都采用交叉验证的方式来预测和评估，选取最优的超参数组合建立模型\n",
    "\n",
    "##### 网格搜索API\n",
    "``````python\n",
    "sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None)\n",
    "    estimator：估计器对象\n",
    "    param_grid：估计器参数(dict){“n_neighbors”:[1,3,5]}\n",
    "    cv：指定几折交叉验证\n",
    "    \n",
    "结果分析：\n",
    "    best_score_:在交叉验证中测试的最好结果\n",
    "    best_estimator_：最好的参数模型\n",
    "    cv_results_:每次交叉验证后的测试集准确率结果和训练集准确率结果\n",
    "``````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validated score of the best_estimator (see below):\n",
      "0.9666666666666667\n",
      "==================================================\n",
      "Best estimator is:\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "==================================================\n",
      "Detailed results are:\n",
      "{'mean_fit_time': array([0.00069294, 0.0004894 , 0.00052092]), 'std_fit_time': array([2.65232546e-04, 7.82706844e-05, 1.08963679e-04]), 'mean_score_time': array([0.00117686, 0.00090501, 0.00099604]), 'std_score_time': array([0.00032406, 0.0001196 , 0.00023415]), 'param_n_neighbors': masked_array(data=[3, 5, 7],\n",
      "             mask=[False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'n_neighbors': 3}, {'n_neighbors': 5}, {'n_neighbors': 7}], 'split0_test_score': array([1., 1., 1.]), 'split1_test_score': array([0.93333333, 0.93333333, 0.93333333]), 'split2_test_score': array([1., 1., 1.]), 'split3_test_score': array([0.93333333, 1.        , 1.        ]), 'split4_test_score': array([0.86666667, 0.86666667, 0.86666667]), 'split5_test_score': array([1.        , 0.93333333, 0.93333333]), 'split6_test_score': array([0.93333333, 0.93333333, 0.93333333]), 'split7_test_score': array([1., 1., 1.]), 'split8_test_score': array([1., 1., 1.]), 'split9_test_score': array([1., 1., 1.]), 'mean_test_score': array([0.96666667, 0.96666667, 0.96666667]), 'std_test_score': array([0.04472136, 0.04472136, 0.04472136]), 'rank_test_score': array([1, 1, 1], dtype=int32), 'split0_train_score': array([0.95555556, 0.96296296, 0.96296296]), 'split1_train_score': array([0.96296296, 0.97037037, 0.97777778]), 'split2_train_score': array([0.95555556, 0.96296296, 0.97037037]), 'split3_train_score': array([0.96296296, 0.97037037, 0.97037037]), 'split4_train_score': array([0.97777778, 0.98518519, 0.98518519]), 'split5_train_score': array([0.95555556, 0.96296296, 0.97037037]), 'split6_train_score': array([0.97037037, 0.97777778, 0.97777778]), 'split7_train_score': array([0.95555556, 0.96296296, 0.97037037]), 'split8_train_score': array([0.95555556, 0.96296296, 0.97037037]), 'split9_train_score': array([0.95555556, 0.97037037, 0.97777778]), 'mean_train_score': array([0.96074074, 0.96888889, 0.97333333]), 'std_train_score': array([0.00744435, 0.00725775, 0.00592593])}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def kNeigh(data):\n",
    "    \n",
    "    # split data into training and test data\n",
    "    # 数据集划分\n",
    "    # 训练数据：用于训练，构建模型\n",
    "    # 测试数据： 用于评估模型是否有效\n",
    "    # train_test_split返回：训练集特征值，测试集特征值，训练集标签，测试集标签\n",
    "    # from sklearn.model_selection import train_test_split\n",
    "    # x_train, x_test, y_train, y_test = train_test_split(data.data, data.target, test_size = 0.25)\n",
    "    \n",
    "    # normalization for both training and test data\n",
    "    # from sklearn.preprocessing import StandardScaler\n",
    "    # ss = StandardScaler()\n",
    "    # x_train = ss.fit_transform(data.data)\n",
    "    # _test = ss.fit_transform(data.target)\n",
    "    \n",
    "    # build model using k nearest neighbors\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    \n",
    "    # grid seach\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "    knn = KNeighborsClassifier()\n",
    "    gs = GridSearchCV(knn, param_grid = {\"n_neighbors\": [3, 5, 7]}, cv = 10)\n",
    "    \n",
    "    gs.fit(data.data, data.target)\n",
    "    #print(gs.predict(x_test))\n",
    "    #print(gs.score(x_test, y_test))\n",
    "    \n",
    "    print(\"Mean cross-validated score of the best_estimator (see below):\")\n",
    "    print(gs.best_score_)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"Best estimator is:\")\n",
    "    print(gs.best_estimator_)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"Detailed results are:\")\n",
    "    print(gs.cv_results_)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    kNeigh(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K近邻算法的优劣\n",
    "- 优势：无需估计参数\n",
    "- 缺点：依赖k值的选取；计算量大，内存开销大"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
